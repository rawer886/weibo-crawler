"""
æµ‹è¯•å•æ¡å¾®åšçš„è¯„è®ºæŠ“å–
"""
import sys
import os
import re
import logging

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from collections import defaultdict
from crawler import WeiboCrawler
from database import save_comment, save_post
from config import LOG_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, LOG_CONFIG["level"]),
    format=LOG_CONFIG["format"],
    handlers=[
        logging.FileHandler(LOG_CONFIG["file"], encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ANSI é¢œè‰²ä»£ç 
class Colors:
    CYAN = '\033[96m'      # é’è‰² - ç”¨äºæ™®é€šç”¨æˆ·å
    YELLOW = '\033[93m'    # é»„è‰² - ç”¨äºåšä¸»åå­—
    RESET = '\033[0m'      # é‡ç½®é¢œè‰²


def display_comments(comments: list):
    """å±•ç¤ºè¯„è®ºï¼ŒæŒ‰çƒ­åº¦æ’åºï¼Œæ”¯æŒæ¥¼å±‚å±•ç¤º

    å‚æ•°:
        comments: è¯„è®ºåˆ—è¡¨
    """
    if not comments:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°è¯„è®º")
        return

    # æ„å»ºè¯„è®ºæ˜ å°„ï¼šcomment_id -> comment
    comment_map = {c['comment_id']: c for c in comments if c.get('comment_id')}

    # æ„å»ºå›å¤å…³ç³»ï¼šè¢«å›å¤çš„è¯„è®ºID -> [å›å¤å®ƒçš„è¯„è®ºåˆ—è¡¨]
    replies_map = defaultdict(list)

    # é¡¶å±‚è¯„è®ºåˆ—è¡¨ï¼ˆæ²¡æœ‰ reply_to_comment_id æˆ–è€…è¢«å›å¤çš„è¯„è®ºä¸å­˜åœ¨çš„ï¼‰
    top_level_comments = []

    for comment in comments:
        reply_to_id = comment.get('reply_to_comment_id')
        if reply_to_id and reply_to_id in comment_map:
            replies_map[reply_to_id].append(comment)
        else:
            top_level_comments.append(comment)

    # é¡¶å±‚è¯„è®ºæŒ‰çƒ­åº¦æ’åº
    top_level_comments.sort(key=lambda x: x.get('likes_count', 0), reverse=True)

    def print_comment(comment, level=0, floor_number=None):
        """æ‰“å°å•æ¡è¯„è®º"""
        indent = "  " * level

        # æ„å»ºç”¨æˆ·ä¿¡æ¯ï¼ˆå¸¦é¢œè‰²ï¼‰
        is_blogger = comment.get('is_blogger_reply', False)
        nickname = comment.get('nickname') or comment.get('uid') or 'æœªçŸ¥ç”¨æˆ·'

        if is_blogger:
            user_info = f"{Colors.YELLOW}{nickname}ğŸ”¥{Colors.RESET}"
        else:
            user_info = f"{Colors.CYAN}{nickname}{Colors.RESET}"

        # ç‚¹èµå’Œæ—¶é—´ä¿¡æ¯
        likes_info = f"ğŸ‘ {comment.get('likes_count', 0)}"
        time_info = comment.get('created_at', 'æœªçŸ¥')

        # é¡¶å±‚è¯„è®º
        if level == 0:
            print(f"{indent}[{floor_number}] {user_info}: {comment.get('content', '')} ({time_info} {likes_info})")
        else:
            # å›å¤è¯„è®º
            reply_to_info = ""
            if comment.get('reply_to_nickname'):
                reply_to_info = f"â†’@{Colors.CYAN}{comment['reply_to_nickname']}{Colors.RESET} "
            print(f"{indent}  â†³ {user_info} {reply_to_info}: {comment.get('content', '')} ({time_info} {likes_info})")

        # é€’å½’æ‰“å°å›å¤ï¼ˆæŒ‰çƒ­åº¦æ’åºï¼‰
        comment_id = comment.get('comment_id')
        if comment_id and comment_id in replies_map:
            sorted_replies = sorted(
                replies_map[comment_id],
                key=lambda x: x.get('likes_count', 0),
                reverse=True
            )
            for reply in sorted_replies:
                print_comment(reply, level + 1)

    # æ‰“å°æ‰€æœ‰é¡¶å±‚è¯„è®ºåŠå…¶å›å¤æ ‘
    for i, comment in enumerate(top_level_comments, 1):
        print_comment(comment, level=0, floor_number=i)


def build_url(url_or_uid: str, mid: str = None) -> str:
    """æ„å»ºå¾®åšURL

    å‚æ•°:
        url_or_uid: å¾®åšURLæˆ–åšä¸»UID
        mid: å¾®åšIDï¼ˆå½“ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯UIDæ—¶éœ€è¦ï¼‰

    è¿”å›:
        å®Œæ•´çš„å¾®åšURL
    """
    if mid is None:
        # å·²ç»æ˜¯URL
        return url_or_uid
    else:
        # uid + mid æ¨¡å¼ï¼Œæ‹¼æ¥URL
        return f"https://weibo.com/{url_or_uid}/{mid}"


def is_numeric_mid(url: str) -> bool:
    """åˆ¤æ–­URLä¸­çš„midæ˜¯å¦ä¸ºæ•°å­—æ ¼å¼"""
    match = re.search(r'weibo\.com/\d+/(\w+)', url)
    if match:
        mid_part = match.group(1)
        return mid_part.isdigit()
    return False


def parse_uid_mid_from_url(url: str) -> tuple[str, str]:
    """ä»URLå­—ç¬¦ä¸²ä¸­è§£æuidå’Œmid"""
    match = re.search(r'weibo\.com/(\d+)/(\w+)', url)
    if match:
        return match.group(1), match.group(2)
    raise ValueError(f"æ— æ³•ä»URLè§£æuidå’Œmid: {url}")


def parse_numeric_mid_from_dom(crawler) -> str:
    """ä»å·²åŠ è½½çš„é¡µé¢DOMä¸­è§£ææ•°å­—æ ¼å¼çš„mid

    è¿”å›:
        æ•°å­—æ ¼å¼çš„mid
    """
    dom_data = crawler.page.evaluate("""
        () => {
            // æ–¹æ³•1: ä» header æ ‡ç­¾è·å– (è¯¦æƒ…é¡µç»“æ„)
            const header = document.querySelector('header[id][userinfo]');
            if (header) {
                const mid = header.getAttribute('id');
                if (mid && /^\\d+$/.test(mid)) {
                    return { mid };
                }
            }

            // æ–¹æ³•2: ä»ä»»æ„å¸¦ mid å±æ€§çš„å…ƒç´ è·å–
            const weiboItem = document.querySelector('[mid]');
            if (weiboItem) {
                const mid = weiboItem.getAttribute('mid');
                return { mid };
            }

            return null;
        }
    """)

    if dom_data and dom_data.get('mid'):
        return dom_data['mid']

    raise ValueError("æ— æ³•ä»DOMè§£ææ•°å­—mid")


def test_single_post_comments(url_or_uid: str, mid: str = None):
    """æµ‹è¯•å•æ¡å¾®åšçš„è¯„è®ºæŠ“å–

    å‚æ•°:
        url_or_uid: å¾®åšURLæˆ–åšä¸»UID
        mid: å¾®åšIDï¼ˆå½“ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯UIDæ—¶éœ€è¦ï¼‰
    """
    crawler = WeiboCrawler()

    try:
        import time

        # === 1. è®¿é—®é¡µé¢ ===
        url = build_url(url_or_uid, mid)
        logger.info(f"è®¿é—®å¾®åšé¡µé¢: {url}")
        crawler.start(url)
        logger.info("ç­‰å¾…5ç§’è®©é¡µé¢å®Œå…¨åŠ è½½...")
        time.sleep(5)
        print()  # ç©ºè¡Œåˆ†éš”

        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not crawler.check_login_status():
            logger.warning("éœ€è¦ç™»å½•...")
            if not crawler.login():
                logger.error("ç™»å½•å¤±è´¥")
                return

        # ä»URLè§£æuidå’Œmid
        uid, mid_from_url = parse_uid_mid_from_url(url)
        if is_numeric_mid(url):
            numeric_mid = mid_from_url
            logger.info(f"UID: {uid}, MID: {numeric_mid}")
        else:
            logger.info(f"æ£€æµ‹åˆ°å¯†æ–‡mid: {mid_from_url}ï¼Œä»é¡µé¢è§£ææ•°å­—mid...")
            try:
                numeric_mid = parse_numeric_mid_from_dom(crawler)
                logger.info(f"UID: {uid}, MID: {numeric_mid} (ä»DOMè§£æ)")
            except ValueError as e:
                logger.error(f"è§£æå¤±è´¥: {e}")
                print("\n" + "=" * 80)
                input("æŒ‰å›è½¦é”®é€€å‡ºæµè§ˆå™¨...")
                return

        # === 2. æŠ“å–å¾®åšæ­£æ–‡ ===
        print()  # ç©ºè¡Œåˆ†éš”
        logger.info("å¼€å§‹æŠ“å–å¾®åšå†…å®¹...")
        post = crawler.parse_post_from_detail_page(uid, numeric_mid)
        if post:
            from database import update_post
            if update_post(post):
                logger.info(f"å¾®åšæ•°æ®å·²æ›´æ–°åˆ°æ•°æ®åº“: {numeric_mid} (å†…å®¹é•¿åº¦={len(post.get('content', ''))}, ç‚¹èµ={post.get('likes_count', 0)})")
            elif save_post(post):
                logger.info(f"å¾®åšå·²ä¿å­˜åˆ°æ•°æ®åº“: {numeric_mid}")
        else:
            logger.warning("æ— æ³•è§£æå¾®åšä¿¡æ¯ï¼Œä»…æŠ“å–è¯„è®º")

        # === 3. æ»‘åŠ¨é¡µé¢ï¼Œç‚¹å‡»ã€ŒæŒ‰çƒ­åº¦ã€æŒ‰é’® ===
        print()  # ç©ºè¡Œåˆ†éš”
        if crawler._scroll_and_wait_for_hot_button():
            time.sleep(2)
            crawler._click_hot_sort_button(scroll_first=False)
        else:
            logger.warning("æœªæ‰¾åˆ°ã€ŒæŒ‰çƒ­åº¦ã€æŒ‰é’®ï¼Œç›´æ¥æŠ“å–è¯„è®º")

        # === 4. æŠ“å–è¯„è®º ===
        logger.info("ç­‰å¾…5ç§’è®©è¯„è®ºåŠ è½½å®Œæˆ...")
        time.sleep(5)
        print()  # ç©ºè¡Œåˆ†éš”
        logger.info(f"å¼€å§‹æŠ“å–å¾®åš {numeric_mid} çš„è¯„è®º...")
        comments = crawler.get_comments(uid, numeric_mid, click_hot_button=False)

        # ä¿å­˜è¯„è®ºï¼ˆæ–°å¢æˆ–æ›´æ–°ç‚¹èµæ•°ï¼‰
        from database import update_comment_likes
        saved_count = 0
        updated_count = 0
        for comment in comments:
            if save_comment(comment):
                saved_count += 1
            else:
                if update_comment_likes(comment["comment_id"], comment.get("likes_count", 0)):
                    updated_count += 1

        logger.info(f"æ–°å¢ {saved_count} æ¡è¯„è®ºï¼Œæ›´æ–° {updated_count} æ¡è¯„è®ºçš„ç‚¹èµæ•°")

        # å±•ç¤ºè¯„è®ºï¼ˆæŒ‰çƒ­åº¦æ’åºï¼Œæ”¯æŒæ¥¼å±‚å±•ç¤ºï¼‰
        print("\n" + "=" * 80)
        print(f"è¯„è®ºåˆ—è¡¨ï¼ˆæŒ‰çƒ­åº¦æ’åºï¼‰ï¼š")
        print("=" * 80)
        display_comments(comments)

        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤åå†é€€å‡º
        print("\n" + "=" * 80)
        input("æŒ‰å›è½¦é”®é€€å‡ºæµè§ˆå™¨...")

    finally:
        crawler.stop()


if __name__ == "__main__":
    if len(sys.argv) < 2:
        logger.error("å‚æ•°è¾“å…¥é”™è¯¯ï¼")
        print("\nç”¨æ³•: python tests/test_single_post.py <url>")
        print("      python tests/test_single_post.py <uid> <mid>")
        print("ç¤ºä¾‹: python tests/test_single_post.py https://weibo.com/2014433131/QoTF4tv2X")
        print("      python tests/test_single_post.py https://weibo.com/1497035431/5256534089008730")
        print("      python tests/test_single_post.py 2014433131 5253489136775271")
        sys.exit(1)

    if len(sys.argv) == 2:
        test_single_post_comments(sys.argv[1])
    else:
        test_single_post_comments(sys.argv[1], sys.argv[2])
